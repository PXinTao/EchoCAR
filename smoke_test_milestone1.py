#!/usr/bin/env python3
"""
é‡Œç¨‹ç¢‘1ç®€åŒ–çƒŸé›¾æµ‹è¯• - åŸºäºç»§æ‰¿çš„RVQé€‚é…å™¨
ç›´æ¥åœ¨VARé¡¹ç›®å†…è¿è¡Œ
"""

import torch
import torch.nn.functional as F
import os
import sys

def test_milestone1_minimal():
    """ç®€åŒ–çš„é‡Œç¨‹ç¢‘1æµ‹è¯•"""
    print("=" * 60)
    print("ğŸš€ é‡Œç¨‹ç¢‘1ï¼šRVQé€‚é…å™¨ - ç®€åŒ–æµ‹è¯•")
    print("=" * 60)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ“± Device: {device}")
    
    # æµ‹è¯•å‚æ•°
    B = 4  # å°batché¿å…å†…å­˜é—®é¢˜
    patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)
    
    try:
        # æ–¹å¼1ï¼šå°è¯•ä»é¢„è®­ç»ƒåˆ›å»ºï¼ˆå¦‚æœæœ‰VAEæƒé‡ï¼‰
        print("\nğŸ”§ Attempting to create from pretrained...")
        try:
            from models.rvq_ultrasound import create_rvq_adapter_from_pretrained
            adapter = create_rvq_adapter_from_pretrained(device=device)
            print("âœ… Created from pretrained VAE")
        except Exception as e:
            print(f"âš ï¸  Pretrained creation failed: {e}")
            print("ğŸ”§ Falling back to minimal creation...")
            from models.rvq_ultrasound import create_rvq_adapter_minimal
            adapter = create_rvq_adapter_minimal(device=device)
            print("âœ… Created minimal adapter")
        
        # åŸºæœ¬ä¿¡æ¯
        total_params = sum(p.numel() for p in adapter.parameters())
        print(f"ğŸ“Š Total parameters: {total_params/1e6:.1f}M")
        
    except Exception as e:
        print(f"âŒ Failed to create adapter: {e}")
        print("ğŸ’¡ Make sure models/rvq_ultrasound.py exists")
        return False
    
    try:
        # æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
        print(f"\nğŸ”§ Testing core functions with batch_size={B}...")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼šç°åº¦è¶…å£°å›¾åƒ
        img = torch.randn(B, 1, 256, 256).to(device)
        print(f"   Input: {img.shape}")
        
        # æµ‹è¯•1: quantize_residual
        z_list, fhat_prefix_list, commit_loss, vq_loss = adapter.quantize_residual(img)
        
        print(f"âœ… quantize_residual:")
        print(f"   z_list: {len(z_list)} scales (expected: {len(patch_nums)})")
        print(f"   fhat_prefix: {len(fhat_prefix_list)} images")
        print(f"   losses: commit={commit_loss.item():.4f}, vq={vq_loss.item():.4f}")
        
        # å¿«é€ŸéªŒè¯å½¢çŠ¶
        assert len(z_list) == len(patch_nums), f"z_list length mismatch"
        assert len(fhat_prefix_list) == len(patch_nums), f"fhat_prefix length mismatch"
        
        for i, (z_s, expected_pn) in enumerate(zip(z_list, patch_nums)):
            expected_len = expected_pn * expected_pn
            assert z_s.shape == (B, expected_len), f"Scale {i} shape mismatch: {z_s.shape} vs {(B, expected_len)}"
        
        for i, fhat in enumerate(fhat_prefix_list):
            assert fhat.shape == (B, 1, 256, 256), f"Prefix {i} shape mismatch: {fhat.shape}"
        
        # æµ‹è¯•2: embed_and_upsample + decode
        f_hat = torch.zeros(B, adapter.quantizer.Cvae, patch_nums[-1], patch_nums[-1]).to(device)
        
        for si, z_s in enumerate(z_list):
            up_feature = adapter.embed_and_upsample(z_s, si)
            f_hat = f_hat + up_feature
        
        final_img = adapter.decode(f_hat)
        
        print(f"âœ… embed_and_upsample + decode:")
        print(f"   cumulative f_hat: {f_hat.shape}")
        print(f"   final_img: {final_img.shape}")
        
        assert final_img.shape == (B, 1, 256, 256), f"Final image shape mismatch: {final_img.shape}"
        
        # æµ‹è¯•3: é‡å»ºè´¨é‡åŸºæœ¬æ£€æŸ¥
        mse = F.mse_loss(final_img, img).item()
        print(f"âœ… Reconstruction MSE: {mse:.6f}")
        
        # ç¡®ä¿æ•°å€¼ç¨³å®š
        assert torch.isfinite(final_img).all(), "Final image contains non-finite values"
        
    except Exception as e:
        print(f"âŒ Core function test failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    # å®Œæˆåº¦æ£€æŸ¥
    print(f"\nâœ… DoD (Definition of Done) æ£€æŸ¥:")
    print(f"   âœ“ len(z_list) == len(scales): {len(z_list)} == {len(patch_nums)}")
    print(f"   âœ“ z_s.shape == [B, g*g] for each scale")
    print(f"   âœ“ len(fhat_prefix_list) == len(scales)")
    print(f"   âœ“ fhat_prefix shape == [B,1,H,W] for each")
    print(f"   âœ“ decode(f_hat) outputs [B,1,256,256]")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ é‡Œç¨‹ç¢‘1 - åŸºäºç»§æ‰¿çš„RVQé€‚é…å™¨æµ‹è¯•é€šè¿‡ï¼")
    print("=" * 60)
    print("\nğŸ”‘ å…³é”®æ¥å£ç¡®è®¤:")
    print("   quantize_residual(img) -> (z_list, fhat_prefix_list, commit_loss, vq_loss)")
    print("   embed_and_upsample(z_s, si) -> up_feature")
    print("   decode(f_hat_latent) -> img")
    print("\nğŸš€ å‡†å¤‡å¥½è¿›å…¥é‡Œç¨‹ç¢‘2ï¼šæ¡ä»¶ç¼–ç å™¨ï¼")
    
    return True


def test_with_var_dependency():
    """å¦‚æœæœ‰å®Œæ•´VARç¯å¢ƒçš„æµ‹è¯•"""
    print("ğŸ” Checking VAR dependencies...")
    
    try:
        from models.vqvae import VQVAE
        from models.quant import VectorQuantizer2
        print("âœ… VAR core modules found")
        return True
    except ImportError as e:
        print(f"âŒ VAR modules not found: {e}")
        print("ğŸ’¡ Please ensure you're running from VAR project root")
        return False


if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    if not test_with_var_dependency():
        print("\nâš ï¸  Please run this script from VAR project root directory")
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    success = test_milestone1_minimal()
    sys.exit(0 if success else 1)